{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from IPython import get_ipython\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Insira seu project token aqui\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # MARATONA BEHIND THE CODE 2020\n","\n"," ## DESAFIO 7 - TNT"]},{"cell_type":"markdown","metadata":{},"source":[" <hr>"]},{"cell_type":"markdown","metadata":{},"source":[" ## Installing Libs"]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Requirement already up-to-date: scikit-learn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.23.2)\nRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.19.1)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (0.16.0)\nRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.5.2)\nRequirement already up-to-date: xgboost in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.2.0)\nRequirement already satisfied, skipping upgrade: numpy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from xgboost) (1.19.1)\nRequirement already satisfied, skipping upgrade: scipy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from xgboost) (1.5.2)\nRequirement already up-to-date: imblearn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.0)\nRequirement already satisfied, skipping upgrade: imbalanced-learn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from imblearn) (0.7.0)\nRequirement already satisfied, skipping upgrade: scikit-learn>=0.23 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (0.23.2)\nRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.2)\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (1.19.1)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (0.16.0)\nRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn>=0.23->imbalanced-learn->imblearn) (2.1.0)\nRequirement already up-to-date: cloudant in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.14.0)\nRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.7.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cloudant) (2.24.0)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.7.0->cloudant) (2020.6.20)\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.7.0->cloudant) (3.0.4)\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.7.0->cloudant) (2.10)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.7.0->cloudant) (1.25.10)\n"}],"source":["get_ipython().system('pip install scikit-learn --upgrade')\n","get_ipython().system('pip install xgboost --upgrade')\n","get_ipython().system('pip install imblearn --upgrade')\n","get_ipython().system('pip install cloudant --upgrade')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" <hr>"]},{"cell_type":"markdown","metadata":{},"source":[" ## Download dos conjuntos de dados em formato .csv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Insira aqui o pandasDataFrame.\n","from cloudant import Cloudant\n","#  from cloudant.client import Cloudant\n","u = '9aa6ed7a-5e4b-4630-b049-4b9994d9365b-bluemix'\n","p = 'd05f4290760adade9c4e3977bbabab948bd5551ee735877e2492f800360f5258'\n","a = '9aa6ed7a-5e4b-4630-b049-4b9994d9365b-bluemix'\n","client = Cloudant(u, p, account=a, connect=True, auto_renew=True)\n","db = client['desafio7_iot_dataset']\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["docs = []\n","response = db.all_docs(limit=500, include_docs=True)\n","for r in response['rows']:\n","    docs.append(r['doc'])\n","type(docs)\n","df = pd.DataFrame(data=docs)\n","df['LAT'] = df['LAT'].astype(float)\n","df['LONG'] = df['LONG'].astype(float)\n","df['Tempo'] = pd.to_datetime(df['Tempo'],errors='coerce')\n","df['year'] = df['Tempo'].dt.year\n","df['month'] = df['Tempo'].dt.month\n","df['day'] = df['Tempo'].dt.day\n","df['weekday'] = df['Tempo'].dt.weekday\n","df['week'] = df['Tempo'].dt.week\n","# trash = ['year','month','day','weekday','week']\n","trash = []\n","remove_trash = ['_id', '_rev','row','Tempo']\n","trash = trash + remove_trash\n","# dummies_column = ['Estação','year','month','day','weekday','week','LAT','LONG']\n","dummies_column = ['Estação','year','month','day','weekday','week','LAT','LONG']\n","\n","df = df.drop(columns=[trash], inplace=False)\n","df.head()\n","# df.info()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.drop_duplicates(subset=None, inplace=True)\n","df.info()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_data_1 = df\n","df_training_dataset = df_data_1\n","df_training_dataset.tail()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Sobre o arquivo \"training_dataset.csv\", temos algumas informações gerais sobre os pontos de vendas da TNT:\n","\n"," **Tempo**\n","\n"," **Estação**\n","\n"," **LAT**\n","\n"," **LONG**\n","\n"," **Movimentação**\n","\n"," **Original_473**\n","\n"," **Original_269**\n","\n"," **Zero**\n","\n"," **Maçã-Verde**\n","\n"," **Tangerina**\n","\n"," **Citrus**\n","\n"," **Açaí-Guaraná**\n","\n"," **Pêssego**\n","\n"," **TARGET**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_training_dataset.info()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_training_dataset.nunique()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" <hr>\n","\n"," ## Detalhamento do desafio: classificação binária\n","\n"," Este é um desafio cujo objetivo de negócio é a segmentação dos usuários de aplicativo de um banco. Para tal, podemos utilizar duas abordagens: aprendizado de máquina supervisionado (classificação) ou não-supervisionado (clustering). Neste desafio será aplicada a classificação, pois é disponível um dataset já com \"labels\", ou em outras palavras, já com exemplos de dados juntamente com a variável alvo.\n","\n"," Na biblioteca scikit-learn temos diversos algoritmos para classificação. O participante é livre para utilizar o framework que desejar para completar esse desafio.\n","\n"," Neste notebook será mostrado um exeplo de uso do algoritmo \"Decision Tree\" para classificar parte dos estudantes em seis diferentes perfís."]},{"cell_type":"markdown","metadata":{},"source":[" # Atenção!\n","\n"," A coluna-alvo neste desafio é a coluna ``TARGET``"]},{"cell_type":"markdown","metadata":{},"source":[" <hr>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Pre-processando o dataset antes do treinamento"]},{"cell_type":"markdown","metadata":{},"source":[" ### Processando valores NaN com o SimpleImputer do sklearn\n","\n"," Para os valores NaN, usaremos a substituição pela constante 0 como **exemplo**.\n","\n"," Você pode escolher a estratégia que achar melhor para tratar os valores nulos :)\n","\n"," Docs: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html?highlight=simpleimputer#sklearn.impute.SimpleImputer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","import numpy as np\n","\n","\n","# impute_zeros = SimpleImputer(\n","#     missing_values=np.nan,\n","#     strategy='constant',\n","#     fill_value=0,\n","#     verbose=0,\n","#     copy=True\n","# )\n","def Missing_Value_imputer():\n","    imputer = SimpleImputer(\n","        missing_values=np.nan\n","        , strategy='most_frequent'\n","        )\n","    return imputer\n","\n","impute_zeros = Missing_Value_imputer()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Exibindo os dados ausentes do conjunto de dados antes da primeira transformação (df)\n","print(\"Valores nulos no df_training_dataset antes da transformação SimpleImputer: \\n\\n{}\\n\".format(df_training_dataset.isnull().sum(axis = 0)))\n","\n","# Aplicando a transformação ``SimpleImputer`` no conjunto de dados base\n","impute_zeros.fit(X=df_training_dataset)\n","\n","# Reconstruindo um Pandas DataFrame com os resultados\n","df_training_dataset_imputed = pd.DataFrame.from_records(\n","    data=impute_zeros.transform(\n","        X=df_training_dataset\n","    ),\n","    columns=df_training_dataset.columns\n",")\n","\n","# Exibindo os dados ausentes do conjunto de dados após a primeira transformação (df)\n","print(\"Valores nulos no df_training_dataset após a transformação SimpleImputer: \\n\\n{}\\n\".format(df_training_dataset_imputed.isnull().sum(axis = 0)))\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Eliminando colunas indesejadas\n","\n"," Vamos **demonstrar** abaixo como usar o método **DataFrame.drop()**.\n","\n"," Docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_training_dataset_imputed.tail()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df_training_dataset_rmcolumns = df_training_dataset_imputed.drop(columns=['Tempo', 'Estação', 'LAT', 'LONG', 'Movimentação'], inplace=False)\n","df_training_dataset_rmcolumns = df_training_dataset_imputed\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_training_dataset_rmcolumns.tail()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Atenção!\n","\n"," As colunas removidas acima são apenas para fim de exemplo, você pode usar as colunas que quiser e inclusive criar novas colunas com dados que achar importantes!\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Tratamento de de variáveis categóricas\n","\n"," Como mencionado antes, os computadores não são bons com variáveis \"categóricas\" (ou strings).\n","\n"," Dado uma coluna com variável categórica, o que podemos realizar é a codificação dessa coluna em múltiplas colunas contendo variáveis binárias. Esse processo é chamado de \"one-hot-encoding\" ou \"dummy encoding\". Se você não é familiarizado com esses termos, você pode pesquisar mais sobre isso na internet :)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tratando variáveis categóricas com o método Pandas ``get_dummies()''\n","# df_training = pd.get_dummies(df_training_dataset_rmcolumns, columns=['Variável a ser aplicado método getDumies()'])\n","df_training = df_training_dataset_rmcolumns\n","df_training.tail()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Atenção!\n","\n"," A coluna **TARGET** deve ser mantida como uma string. Você não precisa processar/codificar a variável-alvo."]},{"cell_type":"markdown","metadata":{},"source":[" <hr>"]},{"cell_type":"markdown","metadata":{},"source":[" ## Treinando um classificador com base em uma árvore de decisão"]},{"cell_type":"markdown","metadata":{},"source":[" ### Selecionando FEATURES e definindo a variável TARGET"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def cat_columns(df_x):\n","    # Categorical boolean mask\n","    categorical_feature_mask = df_x.dtypes==object# filter categorical columns using mask and turn it into a list\n","    categorical_cols = df_x.columns[categorical_feature_mask].tolist()\n","    return categorical_cols\n","\n","feature_columns = cat_columns(df_training_dataset_rmcolumns)\n","feature_columns.pop(feature_columns.index('TARGET'))\n","feature_columns\n","# Tratando variáveis categóricas com o método Pandas ``get_dummies()''\n","df_training = pd.get_dummies(df_training_dataset_rmcolumns, columns=dummies_column)\n","df_training.tail()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_training.columns\n","df_training.columns\n","feature_column2 = df_training.columns.tolist()\n","feature_column2.pop(feature_column2.index('TARGET'))\n","feature_column2\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_training.columns\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = df_training[feature_column2]\n","target = df_training['TARGET']  ## NÃO TROQUE O NOME DA VARIÁVEL TARGET.\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Dividindo nosso conjunto de dados em conjuntos de treinamento e teste"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33, random_state=133)\n","\n","    # %% [markdown]\n","    # ### Treinando uma árvore de decisão"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.preprocessing import Normalizer\n","from sklearn.preprocessing import QuantileTransformer\n","from sklearn.preprocessing import PowerTransformer\n","from sklearn.tree import ExtraTreeClassifier\n","from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n","from sklearn.multioutput import ClassifierChain\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.multiclass import OutputCodeClassifier\n","from sklearn.multiclass import OneVsOneClassifier\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n","from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n","from sklearn.ensemble.bagging import BaggingClassifier\n","from sklearn.ensemble.forest import ExtraTreesClassifier\n","from sklearn.ensemble.forest import RandomForestClassifier\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.semi_supervised import LabelPropagation\n","from sklearn.semi_supervised import LabelSpreading\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import NearestCentroid\n","from sklearn.svm import NuSVC\n","from sklearn.linear_model import Perceptron\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.experimental import enable_hist_gradient_boosting\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","#=================Scaler\n","scaler = [\n","StandardScaler(),\n","MinMaxScaler(),\n","MaxAbsScaler(),\n","RobustScaler(quantile_range=(25, 75)),\n","PowerTransformer(method='yeo-johnson'),\n","# PowerTransformer(method='box-cox'),\n","QuantileTransformer(output_distribution='normal'),\n","QuantileTransformer(output_distribution='uniform'),\n","Normalizer()\n","]\n","\n","\n","#=================Classifier\n","classifier_test = [\n","OneVsRestClassifier(SVC())\n",",DecisionTreeClassifier(max_depth=5)\n",",SVC()\n",",SVC(kernel=\"linear\", C=0.025)\n",",LogisticRegressionCV(cv=5, random_state=0)\n",",GradientBoostingClassifier(random_state=0)\n",",BaggingClassifier(base_estimator=SVC(),n_estimators=10, random_state=0).fit(features, target)\n",",ExtraTreesClassifier(n_estimators=100, random_state=0)\n",",HistGradientBoostingClassifier()\n",",MLPClassifier(random_state=1, max_iter=300)\n",",OneVsOneClassifier(LinearSVC(random_state=0))\n",",OutputCodeClassifier(estimator=RandomForestClassifier(random_state=0),random_state=0)\n","]\n","print('Importacao OK')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def teste(model_scaler,model_classifier,name_scaler,name_classifier,model):\n","    # Método para criar um árvore de decisão\n","    from sklearn.tree import DecisionTreeClassifier\n","\n","\n","    # dtc = DecisionTreeClassifier(max_depth=15).fit(X_train, y_train)\n","    from sklearn.preprocessing import StandardScaler\n","    from sklearn.ensemble import GradientBoostingClassifier\n","    from sklearn.pipeline import Pipeline\n","    from sklearn.ensemble import RandomForestClassifier\n","\n","    # model_scaler = StandardScaler()\n","    # model_classifier = GradientBoostingClassifier(random_state=0)\n","    # model_classifier = RandomForestClassifier()\n","\n","    classifier_model = Pipeline([('scaler', model_scaler), ('svc', model_classifier)])\n","    dtc = classifier_model\n","    print(dtc)\n","    dtc.fit(\n","        X_train,\n","        y_train\n","    )\n","\n","    # %% [markdown]\n","    # ### Fazendo previsões na amostra de teste\n","    y_pred = dtc.predict(X_test)\n","    print(y_pred)\n","\n","    # %% [markdown]\n","    # ### Analisando a qualidade do modelo através da matriz de confusão\n","\n","    import matplotlib.pyplot as plt\n","    import numpy as np\n","    import itertools\n","\n","    def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n","        accuracy = np.trace(cm) / float(np.sum(cm))\n","        misclass = 1 - accuracy\n","        if cmap is None:\n","            cmap = plt.get_cmap('Blues')\n","        plt.figure(figsize=(8, 6))\n","        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","        plt.title(title)\n","        plt.colorbar()\n","        if target_names is not None:\n","            tick_marks = np.arange(len(target_names))\n","            plt.xticks(tick_marks, target_names, rotation=45)\n","            plt.yticks(tick_marks, target_names)\n","        if normalize:\n","            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","            if normalize:\n","                plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n","                        horizontalalignment=\"center\",\n","                        color=\"white\" if cm[i, j] > thresh else \"black\")\n","            else:\n","                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                        horizontalalignment=\"center\",\n","                        color=\"white\" if cm[i, j] > thresh else \"black\")\n","        plt.tight_layout()\n","        plt.ylabel('True label')\n","        plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n","        plt.show()\n","\n","        cm = confusion_matrix(y_test, y_pred)\n","        accuracy = np.trace(cm) / float(np.sum(cm))\n","        end = f'accuracy: {accuracy:.4f} | S: {name_scaler} C: {name_classifier}'\n","        print(end)\n","        dict_test = {}\n","        if accuracy > 0.78:\n","            dict_test[f'M-{model}'] = end\n","\n","\n","        import json\n","        file_name = 'test_result'\n","        with open(f\"{file_name}.json\",\"a\") as file:\n","            json.dump(dict_test, file)\n","\n","        # with open(f\"{file_name}.json\", \"w+\") as file:\n","        #     data = json.load(file)\n","        #     data.update(dict_test)\n","        #     # file.seek(0)\n","        #     json.dump(data, file)\n","\n","    from sklearn.metrics import confusion_matrix\n","\n","\n","    plot_confusion_matrix(confusion_matrix(y_test, y_pred), ['NORMAL', 'REABASTECER'])\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_scaler = StandardScaler()\n","model_classifier = GradientBoostingClassifier(random_state=0)\n","teste(model_scaler,model_classifier,-1,-1,-2)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["count = 0\n","for i in range(len(scaler)):\n","        scaler_i = scaler[i]\n","        for j in range(len(classifier_test)):\n","            count += 1\n","            print(f'Model: {count}')\n","            classifier_test_j = classifier_test[j]\n","            teste(scaler_i,classifier_test_j,i,j,count)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"},"orig_nbformat":2,"kernelspec":{"name":"python_defaultSpec_1599744962733","display_name":"Python 3.8.5 64-bit"}}}