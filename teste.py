# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %% [markdown]
# # Gathered Notebook
# Gathered from ```c:\Users\Lenovo\Desktop\Git Repo\IBM - Behind The Code\desafio-6-2020\notebook.ipynb```
#
# |   |   |
# |---|---|
# |&nbsp;&nbsp;&nbsp|This notebook was generated by the Gather Extension. It requires version 2020.7.94776 (or newer) of the Python Extension, please update [here](https://command:python.datascience.latestExtension). The intent is that it contains only the code and cells required to produce the same results as the cell originally selected for gathering. Please note that the Python analysis is quite conservative, so if it is unsure whether a line of code is necessary for execution, it will err on the side of including it.|
#
# **Are you satisfied with the code that was gathered?**
#
# [Yes](https://command:python.datascience.gatherquality?yes) [No](https://command:python.datascience.gatherquality?no)

# %%
import pandas as pd


# %%
df_training_dataset = pd.read_csv('dataset/training_dataset.csv')


# %%
from sklearn.impute import SimpleImputer
import numpy as np
def Missing_Value_imputer():
    imputer = SimpleImputer(
        missing_values=np.nan
        , strategy='most_frequent'
        )
    return imputer

impute_zeros = Missing_Value_imputer()


# %%
impute_zeros.fit(X=df_training_dataset)
df_training_dataset_imputed = pd.DataFrame.from_records(
    data=impute_zeros.transform(
        X=df_training_dataset
    ),
    columns=df_training_dataset.columns
)


# %%
df_training_dataset_rmcolumns = df_training_dataset_imputed.drop(columns=['id'], inplace=False)


# %%
def cat_columns(df_x):
    # Categorical boolean mask
    categorical_feature_mask = df_x.dtypes==object# filter categorical columns using mask and turn it into a list
    categorical_cols = df_x.columns[categorical_feature_mask].tolist()
    return categorical_cols

feature_columns = cat_columns(df_training_dataset_rmcolumns)
feature_columns.pop(feature_columns.index('categoria'))
feature_columns

# %%
df_training = pd.get_dummies(df_training_dataset_rmcolumns, columns=feature_columns)


# %%
df_training.columns
feature_column2 = df_training.columns.tolist()
feature_column2.pop(feature_column2.index('categoria'))
feature_column2

# Tratando variáveis categóricas com o método Pandas ``get_dummies()''
df_training = pd.get_dummies(df_training_dataset_rmcolumns, columns=feature_columns)
df_training.tail()
df_training.columns
feature_column2 = df_training.columns.tolist()
feature_column2.pop(feature_column2.index('categoria'))
feature_column2
# %%
features = df_training[feature_column2]
target = df_training['categoria']  ## NÃO TROQUE O NOME DA VARIÁVEL TARGET.


# %%
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=133)




# %%
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import PowerTransformer
from sklearn.tree import ExtraTreeClassifier
from sklearn.neural_network.multilayer_perceptron import MLPClassifier
from sklearn.multioutput import ClassifierChain
from sklearn.multioutput import MultiOutputClassifier
from sklearn.multiclass import OutputCodeClassifier
from sklearn.multiclass import OneVsOneClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.ensemble.weight_boosting import AdaBoostClassifier
from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier
from sklearn.ensemble.bagging import BaggingClassifier
from sklearn.ensemble.forest import ExtraTreesClassifier
from sklearn.ensemble.forest import RandomForestClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.calibration import CalibratedClassifierCV
from sklearn.naive_bayes import GaussianNB
from sklearn.semi_supervised import LabelPropagation
from sklearn.semi_supervised import LabelSpreading
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import NearestCentroid
from sklearn.svm import NuSVC
from sklearn.linear_model import Perceptron
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.base import BaseEstimator, TransformerMixin

#=================Scaler
scaler = [
StandardScaler(),
MinMaxScaler(),
MaxAbsScaler(),
RobustScaler(quantile_range=(25, 75)),
PowerTransformer(method='yeo-johnson'),
# PowerTransformer(method='box-cox'),
QuantileTransformer(output_distribution='normal'),
QuantileTransformer(output_distribution='uniform'),
Normalizer()
]


#=================Classifier
classifier_test = [
OneVsRestClassifier(SVC())
,DecisionTreeClassifier(max_depth=5)
,SVC()
,SVC(kernel="linear", C=0.025)
,LogisticRegressionCV(cv=5, random_state=0)
,GradientBoostingClassifier(random_state=0)
,BaggingClassifier(base_estimator=SVC(),n_estimators=10, random_state=0).fit(features, target)
,ExtraTreesClassifier(n_estimators=100, random_state=0)
,HistGradientBoostingClassifier()
,MLPClassifier(random_state=1, max_iter=300)
,OneVsOneClassifier(LinearSVC(random_state=0))
,OutputCodeClassifier(estimator=RandomForestClassifier(random_state=0),random_state=0)
]
print('Importacao OK')

# %%
# =================Looping here

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.pipeline import Pipeline


def super_test(model_scaler,model_classifier,name_scaler,name_classifier,model):
    classifier_model = Pipeline([('scaler', model_scaler), ('svc', model_classifier)])
    dtc = classifier_model
    print(dtc)
    dtc.fit(
        X_train,
        y_train
    )


    y_pred = dtc.predict(X_test)



    import matplotlib.pyplot as plt
    import numpy as np
    import itertools
    def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):
        accuracy = np.trace(cm) / float(np.sum(cm))
        misclass = 1 - accuracy
        if cmap is None:
            cmap = plt.get_cmap('Blues')
        plt.figure(figsize=(8, 6))
        plt.imshow(cm, interpolation='nearest', cmap=cmap)
        plt.title(title)
        plt.colorbar()
        if target_names is not None:
            tick_marks = np.arange(len(target_names))
            plt.xticks(tick_marks, target_names, rotation=45)
            plt.yticks(tick_marks, target_names)
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        thresh = cm.max() / 1.5 if normalize else cm.max() / 2
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            if normalize:
                plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                        horizontalalignment="center",
                        color="white" if cm[i, j] > thresh else "black")
            else:
                plt.text(j, i, "{:,}".format(cm[i, j]),
                        horizontalalignment="center",
                        color="white" if cm[i, j] > thresh else "black")
        plt.tight_layout()
        plt.ylabel('True label')
        plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
        plt.show()




    from sklearn.metrics import confusion_matrix
    plot_confusion_matrix(confusion_matrix(y_test, y_pred), ['parfil1', 'perfil2', 'perfil3', 'perfil4', 'perfil5', 'perfil6'])



    cm = confusion_matrix(y_test, y_pred)
    accuracy = np.trace(cm) / float(np.sum(cm))
    end = f'accuracy: {accuracy:.4f} | S: {name_scaler} C: {name_classifier}'
    print(end)
    dict_test = {}
    if accuracy > .8090:
        dict_test[f'M-{model}'] = end


    import json
    file_name = 'test_result'
    with open(f"{file_name}.json","a") as file:
        json.dump(dict_test, file)
# %%
model_scaler = StandardScaler()
model_classifier = GradientBoostingClassifier(random_state=0)
super_test(model_scaler,model_classifier,-1,-1,-2)

# %%
count = 0
for i in range(len(scaler)):
        scaler_i = scaler[i]
        for j in range(len(classifier_test)):
            count += 1
            print(f'Model: {count}')
            classifier_test_j = classifier_test[j]
            super_test(scaler_i,classifier_test_j,i,j,count)

# %%
